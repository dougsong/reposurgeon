= TO-DO =

== Known bugs ==

* Weird combinations of Subversion copy-delete operations can produce
  content-mismatch failures detectable by repotool compare. In the test
  suite, this problem is exhibited by agito.svn, fleetwood.svn, and
  references.svn.

* Propagation of execute permission in Subversion translations seems flawed.

* Some attempt should be made to create marge links when a deletall is followed
  by a directory copy from another branch. (This is what triggers the "mid-branch
  delete" warning, which should ideally go away.)

* Can gitspace merges be deduced from svk:merge and svnmerge properties?

* Also see https://gitlab.com/esr/reposurgeon/issues for two minor problems.

* The toposort code is back in, but the calls to the resort() function
  in the reorder and reoarent commands have been commented out in both
  Go and Python. The Python version works, the Go version neefs
  fixing. There's a commented-out section documenting toposort,

* repocutter renumber needs a test that demonstrates patching of
  Node-copyfrom-rev fields.

* repocutter needs to patch mergeinfo revisions.

* Restore the ability to work with discontinuous revision sequences.
  Can be tested with a version of swap.svn from before 2019-09-22.

* After the above is done, bring back the too-many-parents.svn test
  from before 2019-09-22 (has both discontinuous revisions and
  mergeinfo properties).

* Fix up generation of R and C in the Mercurial exporter.

* The pathological-load Hg cases fail, that should be fixed.
  They succeeded in Python.

* Need to optimize the allocation code for the main object array;
  appending does way too many copies.

== Testing the code ==

There are tools and makefile productions for finer-grained testing.  
Their behavior is controlled by two variables:

REPOSURGEON: either reposurgeon (the default) or pyreposurgeon

STOPOUT: 1 (the default) produces a bailout after the first failed
test.  Set it to 0 to continue after failed tests - you have to do
this on the make command line, e.g. "cd test; make STOPOUT=0
fi-regress".  Note, the output from this may be voluminous and
unuseful.

To run a single test:

   cd test; ./singletest foo

will check the actual result of the script in foo.tst against its expected
output in foo.chk.  No output other than the test notice is good.  The
Python version passes all these tests.

== Tuning for speed ==

The goal of this port is to improve conversion performance on large
repositories by an order of magnitude or more, with the horrible
example being the GCC subversion history.  Trial runs with the Python
version were taking 9-10 hours!

If you are already a Go expert, you can help by tuning for speed.  The
most important single operations to speed up are fast-import stream reads and
(when the Subversion support is working) Subversion dump stream reads.

First thing to do is make a test load.  The reposurgeon history itself is
large enough to be a useful one.  So:

$ goreposurgeon "read ." "write >reposurgeon.fi"

The ability to dump profile data is built into reposurgeon itself:

$ goreposurgeon "verbose 1" "profile reposurgeon.prof" "read <rs.fi" "profile"

Once you have the profile data you can sic the profile viewer on it.
Have graphviz installed and do

go tool pprof goreposurgeon rs.prof

There are lots of ways to explore the data but the single most interesting one
is to enter "web" and look at your browser. The size of each box is proportional
to the number of profiler samples it appears in.  "top10" gives you the same
data in tabular form:

      flat  flat%   sum%        cum   cum%
     1.41s 22.45% 22.45%      1.43s 22.77%  syscall.Syscall
     1.11s 17.68% 40.13%      2.22s 35.35%  runtime.scanobject
     0.75s 11.94% 52.07%      0.75s 11.94%  runtime.greyobject
     0.58s  9.24% 61.31%      0.58s  9.24%  runtime.memmove

This is telling us that (a) disk I/O (syscall.Syscall) is slow, but garbage
collection overhead dominates (runtime.scanobject and runtime.greyobject, 47%).
That runtime.memmove is probably array copies during append operations.

To go faster we need to exercise the allocator less.  In a way this is
good news - it suggests we don't have a big-O/algorithmic problem.

The obvious thing to do first is a search-and-destroy for heap escapes.
We can't avoid doing a lot of allocation; what we can do is avoid creating
lots of short-lived heap objects that will churn heap storage and trigger GC.

Some references:

https://blog.golang.org/profiling-go-programs

https://github.com/google/pprof/blob/master/doc/README.md

https://www.signalfx.com/blog/a-pattern-for-optimizing-go-2/

ohttp://www.agardner.me/golang/garbage/collection/gc/escape/analysis/2015/10/18/go-escape-analysis.html
